; NOTE: Assertions have been autogenerated by test/update_tpde_llc_test_checks.py UTC_ARGS: --version 5
; SPDX-FileCopyrightText: 2025 Contributors to TPDE <https://tpde.org>
; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

; RUN: tpde-llc --target=x86_64 %s | %objdump | FileCheck %s -check-prefixes=X64
; RUN: tpde-llc --target=aarch64 %s | %objdump | FileCheck %s -check-prefixes=ARM64

define i8 @bitrev_i8(i8 %0) {
; X64-LABEL: <bitrev_i8>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x18
; X64-NEXT:    bswap edi
; X64-NEXT:    mov eax, edi
; X64-NEXT:    and eax, 0xf0f0f0f
; X64-NEXT:    shl eax, 0x4
; X64-NEXT:    shr edi, 0x4
; X64-NEXT:    and edi, 0xf0f0f0f
; X64-NEXT:    or edi, eax
; X64-NEXT:    mov eax, edi
; X64-NEXT:    and eax, 0x33333333
; X64-NEXT:    shr edi, 0x2
; X64-NEXT:    and edi, 0x33333333
; X64-NEXT:    lea eax, [rdi + 4*rax]
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    and ecx, 0x55555555
; X64-NEXT:    shr eax
; X64-NEXT:    and eax, 0x55555555
; X64-NEXT:    lea eax, [rax + 2*rcx]
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <bitrev_i8>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl w0, w0, #24
; ARM64-NEXT:    rbit w0, w0
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i8 @llvm.bitreverse.i8(i8 %0)
  ret i8 %res
}

define i16 @bitrev_i16(i16 %0) {
; X64-LABEL: <bitrev_i16>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x10
; X64-NEXT:    bswap edi
; X64-NEXT:    mov eax, edi
; X64-NEXT:    and eax, 0xf0f0f0f
; X64-NEXT:    shl eax, 0x4
; X64-NEXT:    shr edi, 0x4
; X64-NEXT:    and edi, 0xf0f0f0f
; X64-NEXT:    or edi, eax
; X64-NEXT:    mov eax, edi
; X64-NEXT:    and eax, 0x33333333
; X64-NEXT:    shr edi, 0x2
; X64-NEXT:    and edi, 0x33333333
; X64-NEXT:    lea eax, [rdi + 4*rax]
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    and ecx, 0x55555555
; X64-NEXT:    shr eax
; X64-NEXT:    and eax, 0x55555555
; X64-NEXT:    lea eax, [rax + 2*rcx]
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <bitrev_i16>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl w0, w0, #16
; ARM64-NEXT:    rbit w0, w0
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i16 @llvm.bitreverse.i16(i16 %0)
  ret i16 %res
}

define i23 @bitrev_i23(i23 %0) {
; X64-LABEL: <bitrev_i23>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl edi, 0x9
; X64-NEXT:    bswap edi
; X64-NEXT:    mov eax, edi
; X64-NEXT:    and eax, 0xf0f0f0f
; X64-NEXT:    shl eax, 0x4
; X64-NEXT:    shr edi, 0x4
; X64-NEXT:    and edi, 0xf0f0f0f
; X64-NEXT:    or edi, eax
; X64-NEXT:    mov eax, edi
; X64-NEXT:    and eax, 0x33333333
; X64-NEXT:    shr edi, 0x2
; X64-NEXT:    and edi, 0x33333333
; X64-NEXT:    lea eax, [rdi + 4*rax]
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    and ecx, 0x55555555
; X64-NEXT:    shr eax
; X64-NEXT:    and eax, 0x55555555
; X64-NEXT:    lea eax, [rax + 2*rcx]
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <bitrev_i23>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl w0, w0, #9
; ARM64-NEXT:    rbit w0, w0
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i23 @llvm.bitreverse.i23(i23 %0)
  ret i23 %res
}

define i32 @bitrev_i32(i32 %0) {
; X64-LABEL: <bitrev_i32>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    bswap edi
; X64-NEXT:    mov eax, edi
; X64-NEXT:    and eax, 0xf0f0f0f
; X64-NEXT:    shl eax, 0x4
; X64-NEXT:    shr edi, 0x4
; X64-NEXT:    and edi, 0xf0f0f0f
; X64-NEXT:    or edi, eax
; X64-NEXT:    mov eax, edi
; X64-NEXT:    and eax, 0x33333333
; X64-NEXT:    shr edi, 0x2
; X64-NEXT:    and edi, 0x33333333
; X64-NEXT:    lea eax, [rdi + 4*rax]
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    and ecx, 0x55555555
; X64-NEXT:    shr eax
; X64-NEXT:    and eax, 0x55555555
; X64-NEXT:    lea eax, [rax + 2*rcx]
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <bitrev_i32>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    rbit w0, w0
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i32 @llvm.bitreverse.i32(i32 %0)
  ret i32 %res
}

define i37 @bitrev_i37(i37 %0) {
; X64-LABEL: <bitrev_i37>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    shl rdi, 0x1b
; X64-NEXT:    bswap rdi
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    shr rax, 0x4
; X64-NEXT:    movabs rcx, 0xf0f0f0f0f0f0f0f
; X64-NEXT:    and rax, rcx
; X64-NEXT:    and rdi, rcx
; X64-NEXT:    shl rdi, 0x4
; X64-NEXT:    or rdi, rax
; X64-NEXT:    movabs rax, 0x3333333333333333
; X64-NEXT:    mov rcx, rdi
; X64-NEXT:    and rcx, rax
; X64-NEXT:    shr rdi, 0x2
; X64-NEXT:    and rdi, rax
; X64-NEXT:    lea rax, [rdi + 4*rcx]
; X64-NEXT:    movabs rcx, 0x5555555555555555
; X64-NEXT:    mov rdx, rax
; X64-NEXT:    and rdx, rcx
; X64-NEXT:    shr rax
; X64-NEXT:    and rax, rcx
; X64-NEXT:    lea rax, [rax + 2*rdx]
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <bitrev_i37>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    lsl x0, x0, #27
; ARM64-NEXT:    rbit x0, x0
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i37 @llvm.bitreverse.i37(i37 %0)
  ret i37 %res
}

define i64 @bitrev_i64(i64 %0) {
; X64-LABEL: <bitrev_i64>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    bswap rdi
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    shr rax, 0x4
; X64-NEXT:    movabs rcx, 0xf0f0f0f0f0f0f0f
; X64-NEXT:    and rax, rcx
; X64-NEXT:    and rdi, rcx
; X64-NEXT:    shl rdi, 0x4
; X64-NEXT:    or rdi, rax
; X64-NEXT:    movabs rax, 0x3333333333333333
; X64-NEXT:    mov rcx, rdi
; X64-NEXT:    and rcx, rax
; X64-NEXT:    shr rdi, 0x2
; X64-NEXT:    and rdi, rax
; X64-NEXT:    lea rax, [rdi + 4*rcx]
; X64-NEXT:    movabs rcx, 0x5555555555555555
; X64-NEXT:    mov rdx, rax
; X64-NEXT:    and rdx, rcx
; X64-NEXT:    shr rax
; X64-NEXT:    and rax, rcx
; X64-NEXT:    lea rax, [rax + 2*rdx]
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <bitrev_i64>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    rbit x0, x0
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %res = call i64 @llvm.bitreverse.i64(i64 %0)
  ret i64 %res
}

