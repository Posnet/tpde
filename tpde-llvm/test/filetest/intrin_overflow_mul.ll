; NOTE: Assertions have been autogenerated by utils/update_tpde_llvm_test_checks.py UTC_ARGS: --tool tpde_llvm --default-march x86-64-v2 --filter-out "int3" --version 5
; SPDX-FileCopyrightText: 2024 Tobias Schwarz <tobias.schwarz@tum.de>
;
; SPDX-License-Identifier: LicenseRef-Proprietary

; RUN: tpde_llvm %s | llvm-objdump -d -r --no-show-raw-insn --symbolize-operands --no-addresses --x86-asm-syntax=intel - | FileCheck %s -check-prefixes=X64,CHECK --enable-var-scope --dump-input always


declare {i8, i1} @llvm.smul.with.overflow.i8(i8, i8)
declare {i16, i1} @llvm.smul.with.overflow.i16(i16, i16)
declare {i32, i1} @llvm.smul.with.overflow.i32(i32, i32)
declare {i64, i1} @llvm.smul.with.overflow.i64(i64, i64)
declare {i128, i1} @llvm.smul.with.overflow.i128(i128, i128)

declare {i8, i1} @llvm.umul.with.overflow.i8(i8, i8)
declare {i16, i1} @llvm.umul.with.overflow.i16(i16, i16)
declare {i32, i1} @llvm.umul.with.overflow.i32(i32, i32)
declare {i64, i1} @llvm.umul.with.overflow.i64(i64, i64)
declare {i128, i1} @llvm.umul.with.overflow.i128(i128, i128)

define i8 @umul_i8_0(i8 %0, i8 %1) {
; X64-LABEL: umul_i8_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x30
; X64:    xor ecx, ecx
; X64:    mov eax, edi
; X64:    mul sil
; X64:    seto cl
; X64:    add rsp, 0x30
; X64:    pop rbp
; X64:    ret
entry:
  %2 = call {i8, i1} @llvm.umul.with.overflow.i8(i8 %0, i8 %1)
  %3 = extractvalue {i8, i1} %2, 0
  ret i8 %3
}

define i1 @umul_i8_1(i8 %0, i8 %1) {
; X64-LABEL: umul_i8_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x30
; X64:    xor ecx, ecx
; X64:    mov eax, edi
; X64:    mul sil
; X64:    seto cl
; X64:    mov eax, ecx
; X64:    add rsp, 0x30
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rax], al
entry:
  %2 = call {i8, i1} @llvm.umul.with.overflow.i8(i8 %0, i8 %1)
  %3 = extractvalue {i8, i1} %2, 1
  ret i1 %3
}

define i16 @umul_i16_0(i16 %0, i16 %1) {
; X64-LABEL: umul_i16_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x30
; X64:    xor ecx, ecx
; X64:    mov eax, edi
; X64:    mul si
; X64:    seto cl
; X64:    add rsp, 0x30
; X64:    pop rbp
; X64:    ret
entry:
  %2 = call {i16, i1} @llvm.umul.with.overflow.i16(i16 %0, i16 %1)
  %3 = extractvalue {i16, i1} %2, 0
  ret i16 %3
}

define i1 @umul_i16_1(i16 %0, i16 %1) {
; X64-LABEL: umul_i16_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x40
; X64:    xor ecx, ecx
; X64:    mov eax, edi
; X64:    mul si
; X64:    seto cl
; X64:    mov eax, ecx
; X64:    add rsp, 0x40
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rax], al
entry:
  %2 = call {i16, i1} @llvm.umul.with.overflow.i16(i16 %0, i16 %1)
  %3 = extractvalue {i16, i1} %2, 1
  ret i1 %3
}

define i32 @umul_i32_0(i32 %0, i32 %1) {
; X64-LABEL: umul_i32_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x40
; X64:    xor ecx, ecx
; X64:    mov eax, edi
; X64:    mul esi
; X64:    seto cl
; X64:    add rsp, 0x40
; X64:    pop rbp
; X64:    ret
; X64:    add byte ptr [rbp + 0x48], dl
entry:
  %2 = call {i32, i1} @llvm.umul.with.overflow.i32(i32 %0, i32 %1)
  %3 = extractvalue {i32, i1} %2, 0
  ret i32 %3
}

define i1 @umul_i32_1(i32 %0, i32 %1) {
; X64-LABEL: umul_i32_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x40
; X64:    xor ecx, ecx
; X64:    mov eax, edi
; X64:    mul esi
; X64:    seto cl
; X64:    mov eax, ecx
; X64:    add rsp, 0x40
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rax], al
; X64:    add byte ptr [rbp + 0x48], dl
entry:
  %2 = call {i32, i1} @llvm.umul.with.overflow.i32(i32 %0, i32 %1)
  %3 = extractvalue {i32, i1} %2, 1
  ret i1 %3
}

define i64 @umul_i64_0(i64 %0, i64 %1) {
; X64-LABEL: umul_i64_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x50
; X64:    xor ecx, ecx
; X64:    mov rax, rdi
; X64:    mul rsi
; X64:    seto cl
; X64:    add rsp, 0x50
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rax], al
; X64:    add byte ptr [rbp + 0x48], dl
entry:
  %2 = call {i64, i1} @llvm.umul.with.overflow.i64(i64 %0, i64 %1)
  %3 = extractvalue {i64, i1} %2, 0
  ret i64 %3
}

define i1 @umul_i64_1(i64 %0, i64 %1) {
; X64-LABEL: umul_i64_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x60
; X64:    xor ecx, ecx
; X64:    mov rax, rdi
; X64:    mul rsi
; X64:    seto cl
; X64:    mov eax, ecx
; X64:    add rsp, 0x60
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rbp + 0x48], dl
entry:
  %2 = call {i64, i1} @llvm.umul.with.overflow.i64(i64 %0, i64 %1)
  %3 = extractvalue {i64, i1} %2, 1
  ret i1 %3
}

define i128 @umul_i128_0(i128 %0, i128 %1) {
; X64-LABEL: umul_i128_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    push rbx
; X64:    nop dword ptr [rax + rax]
; X64:    sub rsp, 0x68
; X64:    mov qword ptr [rbp - 0x50], rdx
; X64:    test rcx, rcx
; X64:    setne al
; X64:    test rsi, rsi
; X64:    setne bl
; X64:    and bl, al
; X64:    mov rax, rsi
; X64:    mul qword ptr [rbp - 0x50]
; X64:    seto r8b
; X64:    mov r9, rax
; X64:    mov rax, rcx
; X64:    mul rdi
; X64:    seto r10b
; X64:    or r10b, r8b
; X64:    or r10b, bl
; X64:    add r9, rax
; X64:    mov rax, rdi
; X64:    mul qword ptr [rbp - 0x50]
; X64:    add r9, rdx
; X64:    setb dl
; X64:    or dl, r10b
; X64:    movzx edx, dl
; X64:    mov rdx, r9
; X64:    add rsp, 0x68
; X64:    pop rbx
; X64:    pop rbp
; X64:    ret
; X64:    add byte ptr [rbp + 0x48], dl
entry:
  %2 = call {i128, i1} @llvm.umul.with.overflow.i128(i128 %0, i128 %1)
  %3 = extractvalue {i128, i1} %2, 0
  ret i128 %3
}

define i1 @umul_i128_1(i128 %0, i128 %1) {
; X64-LABEL: umul_i128_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    push rbx
; X64:    nop dword ptr [rax + rax]
; X64:    sub rsp, 0x78
; X64:    mov qword ptr [rbp - 0x50], rdx
; X64:    test rcx, rcx
; X64:    setne al
; X64:    test rsi, rsi
; X64:    setne bl
; X64:    and bl, al
; X64:    mov rax, rsi
; X64:    mul qword ptr [rbp - 0x50]
; X64:    seto r8b
; X64:    mov r9, rax
; X64:    mov rax, rcx
; X64:    mul rdi
; X64:    seto r10b
; X64:    or r10b, r8b
; X64:    or r10b, bl
; X64:    add r9, rax
; X64:    mov rax, rdi
; X64:    mul qword ptr [rbp - 0x50]
; X64:    add r9, rdx
; X64:    setb dl
; X64:    or dl, r10b
; X64:    movzx edx, dl
; X64:    mov eax, edx
; X64:    add rsp, 0x78
; X64:    pop rbx
; X64:    pop rbp
; X64:    ret
; X64:    add byte ptr [rax], al
entry:
  %2 = call {i128, i1} @llvm.umul.with.overflow.i128(i128 %0, i128 %1)
  %3 = extractvalue {i128, i1} %2, 1
  ret i1 %3
}



define i8 @smul_i8_0(i8 %0, i8 %1) {
; X64-LABEL: smul_i8_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x30
; X64:    xor ecx, ecx
; X64:    mov eax, edi
; X64:    imul sil
; X64:    seto cl
; X64:    add rsp, 0x30
; X64:    pop rbp
; X64:    ret
entry:
  %2 = call {i8, i1} @llvm.smul.with.overflow.i8(i8 %0, i8 %1)
  %3 = extractvalue {i8, i1} %2, 0
  ret i8 %3
}

define i1 @smul_i8_1(i8 %0, i8 %1) {
; X64-LABEL: smul_i8_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x30
; X64:    xor ecx, ecx
; X64:    mov eax, edi
; X64:    imul sil
; X64:    seto cl
; X64:    mov eax, ecx
; X64:    add rsp, 0x30
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rax], al
entry:
  %2 = call {i8, i1} @llvm.smul.with.overflow.i8(i8 %0, i8 %1)
  %3 = extractvalue {i8, i1} %2, 1
  ret i1 %3
}

define i16 @smul_i16_0(i16 %0, i16 %1) {
; X64-LABEL: smul_i16_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x30
; X64:    xor eax, eax
; X64:    imul di, si
; X64:    seto al
; X64:    mov eax, edi
; X64:    add rsp, 0x30
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rax], al
; X64:    add byte ptr [rbp + 0x48], dl
entry:
  %2 = call {i16, i1} @llvm.smul.with.overflow.i16(i16 %0, i16 %1)
  %3 = extractvalue {i16, i1} %2, 0
  ret i16 %3
}

define i1 @smul_i16_1(i16 %0, i16 %1) {
; X64-LABEL: smul_i16_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x40
; X64:    xor eax, eax
; X64:    imul di, si
; X64:    seto al
; X64:    add rsp, 0x40
; X64:    pop rbp
; X64:    ret
; X64:    add byte ptr [rbp + 0x48], dl
entry:
  %2 = call {i16, i1} @llvm.smul.with.overflow.i16(i16 %0, i16 %1)
  %3 = extractvalue {i16, i1} %2, 1
  ret i1 %3
}

define i32 @smul_i32_0(i32 %0, i32 %1) {
; X64-LABEL: smul_i32_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x40
; X64:    xor eax, eax
; X64:    imul edi, esi
; X64:    seto al
; X64:    mov eax, edi
; X64:    add rsp, 0x40
; X64:    pop rbp
; X64:    ret
entry:
  %2 = call {i32, i1} @llvm.smul.with.overflow.i32(i32 %0, i32 %1)
  %3 = extractvalue {i32, i1} %2, 0
  ret i32 %3
}

define i1 @smul_i32_1(i32 %0, i32 %1) {
; X64-LABEL: smul_i32_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x40
; X64:    xor eax, eax
; X64:    imul edi, esi
; X64:    seto al
; X64:    add rsp, 0x40
; X64:    pop rbp
; X64:    ret
; X64:    add byte ptr [rax], al
entry:
  %2 = call {i32, i1} @llvm.smul.with.overflow.i32(i32 %0, i32 %1)
  %3 = extractvalue {i32, i1} %2, 1
  ret i1 %3
}

define i64 @smul_i64_0(i64 %0, i64 %1) {
; X64-LABEL: smul_i64_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x50
; X64:    xor eax, eax
; X64:    imul rdi, rsi
; X64:    seto al
; X64:    mov rax, rdi
; X64:    add rsp, 0x50
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rax], al
entry:
  %2 = call {i64, i1} @llvm.smul.with.overflow.i64(i64 %0, i64 %1)
  %3 = extractvalue {i64, i1} %2, 0
  ret i64 %3
}

define i1 @smul_i64_1(i64 %0, i64 %1) {
; X64-LABEL: smul_i64_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    nop word ptr [rax + rax]
; X64:    sub rsp, 0x60
; X64:    xor eax, eax
; X64:    imul rdi, rsi
; X64:    seto al
; X64:    add rsp, 0x60
; X64:    pop rbp
; X64:    ret
; X64:    add byte ptr [rbp + 0x48], dl
entry:
  %2 = call {i64, i1} @llvm.smul.with.overflow.i64(i64 %0, i64 %1)
  %3 = extractvalue {i64, i1} %2, 1
  ret i1 %3
}

define i128 @smul_i128_0(i128 %0, i128 %1) {
; X64-LABEL: smul_i128_0>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    push rbx
; X64:    push r12
; X64:    push r13
; X64:    push r14
; X64:    push r15
; X64:    sub rsp, 0x48
; X64:    mov qword ptr [rbp - 0x50], rdx
; X64:    mov rbx, rsi
; X64:    sar rbx, 0x3f
; X64:    mov r8, rcx
; X64:    imul r8, rbx
; X64:    mov r9, qword ptr [rbp - 0x50]
; X64:    mov rax, r9
; X64:    mul rbx
; X64:    mov rbx, rdx
; X64:    add rbx, rax
; X64:    add rbx, r8
; X64:    mov r10, rax
; X64:    mov rax, rcx
; X64:    sar rax, 0x3f
; X64:    mov r11, rax
; X64:    imul r11, rsi
; X64:    mul rdi
; X64:    mov r8, rdx
; X64:    add r8, r11
; X64:    add r8, rax
; X64:    mov r12, rax
; X64:    add r12, r10
; X64:    adc r8, rbx
; X64:    mov rax, rdi
; X64:    mul r9
; X64:    mov r10, rax
; X64:    mov rbx, rdx
; X64:    mov rax, rsi
; X64:    mul r9
; X64:    add rbx, rax
; X64:    mov r13, rdx
; X64:    adc r13, 0x0
; X64:    mov rax, rdi
; X64:    mul rcx
; X64:    add rbx, rax
; X64:    mov r14, rdx
; X64:    adc r14, r13
; X64:    setb al
; X64:    movzx r13d, al
; X64:    mov rax, rsi
; X64:    mul rcx
; X64:    add rax, r14
; X64:    adc rdx, r13
; X64:    add rax, r12
; X64:    adc rdx, r8
; X64:    mov r15, rbx
; X64:    sar r15, 0x3f
; X64:    xor rdx, r15
; X64:    xor r15, rax
; X64:    xor r8d, r8d
; X64:    or r15, rdx
; X64:    setne r8b
; X64:    mov rax, r10
; X64:    mov rdx, rbx
; X64:    add rsp, 0x48
; X64:    pop r15
; X64:    pop r14
; X64:    pop r13
; X64:    pop r12
; X64:    pop rbx
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    add byte ptr [rax], al
entry:
  %2 = call {i128, i1} @llvm.smul.with.overflow.i128(i128 %0, i128 %1)
  %3 = extractvalue {i128, i1} %2, 0
  ret i128 %3
}

define i1 @smul_i128_1(i128 %0, i128 %1) {
; X64-LABEL: smul_i128_1>:
; X64:    push rbp
; X64:    mov rbp, rsp
; X64:    push rbx
; X64:    push r12
; X64:    push r13
; X64:    push r14
; X64:    push r15
; X64:    sub rsp, 0x58
; X64:    mov qword ptr [rbp - 0x50], rdx
; X64:    mov rbx, rsi
; X64:    sar rbx, 0x3f
; X64:    mov r8, rcx
; X64:    imul r8, rbx
; X64:    mov r9, qword ptr [rbp - 0x50]
; X64:    mov rax, r9
; X64:    mul rbx
; X64:    mov rbx, rdx
; X64:    add rbx, rax
; X64:    add rbx, r8
; X64:    mov r10, rax
; X64:    mov rax, rcx
; X64:    sar rax, 0x3f
; X64:    mov r11, rax
; X64:    imul r11, rsi
; X64:    mul rdi
; X64:    mov r8, rdx
; X64:    add r8, r11
; X64:    add r8, rax
; X64:    mov r12, rax
; X64:    add r12, r10
; X64:    adc r8, rbx
; X64:    mov rax, rdi
; X64:    mul r9
; X64:    mov r10, rax
; X64:    mov rbx, rdx
; X64:    mov rax, rsi
; X64:    mul r9
; X64:    add rbx, rax
; X64:    mov r13, rdx
; X64:    adc r13, 0x0
; X64:    mov rax, rdi
; X64:    mul rcx
; X64:    add rbx, rax
; X64:    mov r14, rdx
; X64:    adc r14, r13
; X64:    setb al
; X64:    movzx r13d, al
; X64:    mov rax, rsi
; X64:    mul rcx
; X64:    add rax, r14
; X64:    adc rdx, r13
; X64:    add rax, r12
; X64:    adc rdx, r8
; X64:    mov r15, rbx
; X64:    sar r15, 0x3f
; X64:    xor rdx, r15
; X64:    xor r15, rax
; X64:    xor r8d, r8d
; X64:    or r15, rdx
; X64:    setne r8b
; X64:    mov eax, r8d
; X64:    add rsp, 0x58
; X64:    pop r15
; X64:    pop r14
; X64:    pop r13
; X64:    pop r12
; X64:    pop rbx
; X64:    pop rbp
; X64:    ret
; X64:     ...
; X64:    <unknown>
entry:
  %2 = call {i128, i1} @llvm.smul.with.overflow.i128(i128 %0, i128 %1)
  %3 = extractvalue {i128, i1} %2, 1
  ret i1 %3
}
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; CHECK: {{.*}}
